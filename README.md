# ğŸ¤Ÿ AI Libras (Brazilian Sign Language) Alphabet Interpreter

An AI-powered system that interprets the **Brazilian Sign Language (Libras) alphabet** using computer vision and machine learning.  
The application leverages **MediaPipe** for hand landmark detection and a **Random Forest classifier** to recognize letters in real time through a webcam.  

This project aims to contribute to **accessibility and inclusion**, providing a foundation that can be extended to full word and sentence recognition in the future.  

---

## ğŸ“Œ Features
- ğŸ”¹ Real-time recognition of Libras alphabet letters via webcam  
- ğŸ”¹ Hand landmark detection powered by Google **MediaPipe**  
- ğŸ”¹ Machine learning classification with **Random Forest**  
- ğŸ”¹ Visual feedback: bounding box and predicted letter displayed on screen  

---

## ğŸ› ï¸ Technologies Used
- **Python** â€“ main programming language  
- **OpenCV** â€“ image processing and webcam capture  
- **MediaPipe** â€“ hand tracking and landmark detection  
- **scikit-learn** â€“ machine learning model training (Random Forest)  
- **NumPy** â€“ data handling and preprocessing  

---

## ğŸ“‚ Project Structure
```

.
â”œâ”€â”€ data/                # Training dataset (organized in folders by letter)
â”œâ”€â”€ models/              # Saved ML model
â”œâ”€â”€ training.py          # Train the Random Forest classifier
â”œâ”€â”€ collect\_data.py      # Process images and generate training dataset
â”œâ”€â”€ real\_time.py         # Real-time prediction with webcam
â”œâ”€â”€ requirements.txt     # Dependencies
â””â”€â”€ README.md            # Project documentation

````

---

## ğŸš€ Installation & Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/usuario/repo.git
   cd repo
````

2. **Create a virtual environment (recommended)**

   ```bash
   python -m venv venv
   source venv/bin/activate   # Linux/Mac
   venv\Scripts\activate      # Windows
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

---

## â–¶ï¸ Usage

1. **Prepare dataset**
   Place images of hands performing Libras alphabet signs inside `./data/` with one folder per letter.

2. **Generate training data**

   ```bash
   python collect_data.py
   ```

3. **Train the model**

   ```bash
   python training.py
   ```

4. **Run real-time prediction**

   ```bash
   python real_time.py
   ```

---

## ğŸ“Š Results

* Achieved **high accuracy** in recognizing the Libras alphabet during testing
* Works in **real-time** with low latency
* Robust under different lighting conditions (but still room for improvement)

---

## âš¡ Challenges

* Building a consistent dataset with enough variety of hand positions and lighting conditions
* Maintaining real-time performance while processing landmarks and predictions

---

## ğŸŒ± Future Improvements

* Expand dataset to support **words and full sentences** in Libras
* Optimize the model for **mobile devices**
* Add **voice output** for recognized letters/words
* Improve UI/UX for accessibility

---

## ğŸ“œ License

This project is released under the [MIT License](LICENSE).

---

## ğŸ‘¨â€ğŸ’» Author

Developed by **[Vitor G. J. de Carvalho](https://github.com/usuario)**

